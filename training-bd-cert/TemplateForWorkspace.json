{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "training-bd-cert"
		},
		"Data_Warehouse_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'Data_Warehouse'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=training-bd-cert.sql.azuresynapse.net;Initial Catalog=romandp203"
		},
		"romandp203_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'romandp203'"
		},
		"training-bd-cert-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'training-bd-cert-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:training-bd-cert.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"Products_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/01/adventureworks/products.csv"
		},
		"romandp203_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://romandp203.dfs.core.windows.net/"
		},
		"training-bd-cert-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bdaadlssandbox00.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/romandp203')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Load Product Data')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "LoadProducts",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "LoadProductsData",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"ProductsText": {},
									"ProductTable": {},
									"DimProductTable": {}
								}
							},
							"staging": {
								"linkedService": {
									"referenceName": "romandp203",
									"type": "LinkedServiceReference"
								},
								"folderPath": "files/stage_products"
							},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/LoadProductsData')]",
				"[concat(variables('workspaceId'), '/linkedServices/romandp203')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DimProduct')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "Data_Warehouse",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "ProductKey",
						"type": "int",
						"precision": 10
					},
					{
						"name": "ProductAltKey",
						"type": "nvarchar"
					},
					{
						"name": "ProductName",
						"type": "nvarchar"
					},
					{
						"name": "Color",
						"type": "nvarchar"
					},
					{
						"name": "Size",
						"type": "nvarchar"
					},
					{
						"name": "ListPrice",
						"type": "money",
						"precision": 19,
						"scale": 4
					},
					{
						"name": "Discontinued",
						"type": "bit"
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "DimProduct"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/Data_Warehouse')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Products_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "romandp203",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "Product.csv",
						"folderPath": "data",
						"fileSystem": "files"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ProductID",
						"type": "String"
					},
					{
						"name": "ProductName",
						"type": "String"
					},
					{
						"name": "Color",
						"type": "String"
					},
					{
						"name": "Size",
						"type": "String"
					},
					{
						"name": "ListPrice",
						"type": "String"
					},
					{
						"name": "Discontinued",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/romandp203')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data_Warehouse')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Dedicated SQL pool",
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('Data_Warehouse_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Products')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Product list via HTTP",
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('Products_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/romandp203')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('romandp203_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('romandp203_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/training-bd-cert-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('training-bd-cert-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/training-bd-cert-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('training-bd-cert-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadProductsData')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Products_Csv",
								"type": "DatasetReference"
							},
							"name": "ProductsText",
							"description": "Products text data"
						},
						{
							"dataset": {
								"referenceName": "DimProduct",
								"type": "DatasetReference"
							},
							"name": "ProductTable",
							"description": "Product table"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DimProduct",
								"type": "DatasetReference"
							},
							"name": "DimProductTable",
							"description": "Load DimProduct table"
						}
					],
					"transformations": [
						{
							"name": "MatchedProducts",
							"description": "Matched product data"
						},
						{
							"name": "SetLoadAction",
							"description": "Insert new, upsert existing"
						}
					],
					"scriptLines": [
						"source(output(",
						"          ProductID as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as decimal(10,0),",
						"          Discontinued as boolean",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> ProductsText",
						"source(output(",
						"          ProductKey as integer,",
						"          ProductAltKey as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as decimal(19,4),",
						"          Discontinued as boolean",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table',",
						"     staged: true) ~> ProductTable",
						"ProductsText, ProductTable lookup(ProductID == ProductAltKey,",
						"     multiple: false,",
						"     pickup: 'last',",
						"     asc(ProductKey, true),",
						"     broadcast: 'auto')~> MatchedProducts",
						"MatchedProducts alterRow(insertIf(isNull(ProductKey)),",
						"     upsertIf(not(isNull(ProductKey)))) ~> SetLoadAction",
						"SetLoadAction sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          ProductKey as integer,",
						"          ProductAltKey as string,",
						"          ProductName as string,",
						"          Color as string,",
						"          Size as string,",
						"          ListPrice as decimal(19,4),",
						"          Discontinued as boolean",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:true,",
						"     keys:['ProductAltKey'],",
						"     format: 'table',",
						"     staged: true,",
						"     allowCopyCommand: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          ProductAltKey = ProductID,",
						"          ProductName = ProductsText@ProductName,",
						"          Color = ProductsText@Color,",
						"          Size = ProductsText@Size,",
						"          ListPrice = ProductsText@ListPrice,",
						"          Discontinued = ProductsText@Discontinued",
						"     )) ~> DimProductTable"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Products_Csv')]",
				"[concat(variables('workspaceId'), '/datasets/DimProduct')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create ProductSalesTotals table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE EXTERNAL TABLE ProductSalesTotal\n    WITH (\n        LOCATION= 'sales/productsales',\n        DATA_SOURCE = sales_data,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT Item AS Product,\n       SUM(Quantity) AS ItemsSold,\n       ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\nFROM\n    OPENROWSET(\n        BULK 'sales/csv/*.csv',\n        DATA_SOURCE='sales_data',\n        FORMAT='CSV',\n        PARSER_VERSION='2.0',\n        HEADER_ROW=TRUE\n    ) AS orders\nGROUP BY Item;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": -1
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create Sales DB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " -- Database for sales data\n CREATE DATABASE Sales\n   COLLATE Latin1_General_100_BIN2_UTF8;\n GO;\n    \n Use Sales;\n GO;\n    \n -- External data is in the Files container in the data lake\n CREATE EXTERNAL DATA SOURCE sales_data WITH (\n     LOCATION = 'https://romandp203.dfs.core.windows.net/files/'\n );\n GO;\n    \n -- Format for table files\n CREATE EXTERNAL FILE FORMAT ParquetFormat\n     WITH (\n             FORMAT_TYPE = PARQUET,\n             DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n         );\n GO;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create YearlySalesTotals table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "USE Sales;\nGO;\nCREATE PROCEDURE sp_GetYearlySales\nAS\nBEGIN\n    -- drop existing table\n    IF EXISTS (\n            SELECT * FROM sys.external_tables\n            WHERE name = 'YearlySalesTotals'\n        )\n        DROP EXTERNAL TABLE YearlySalesTotals\n    -- create external table\n    CREATE EXTERNAL TABLE YearlySalesTotals\n    WITH (\n            LOCATION = 'sales/yearlysales/',\n            DATA_SOURCE = sales_data,\n            FILE_FORMAT = ParquetFormat\n        )\n    AS\n    SELECT YEAR(OrderDate) AS CalendarYear,\n            SUM(Quantity) AS ItemsSold,\n            ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\n    FROM\n        OPENROWSET(\n            BULK 'sales/csv/*.csv',\n            DATA_SOURCE = 'sales_data',\n            FORMAT = 'CSV',\n            PARSER_VERSION = '2.0',\n            HEADER_ROW = TRUE\n        ) AS orders\n    GROUP BY YEAR(OrderDate)\nEND\n\nEXEC sp_GetYearlySales;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query Sales CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://romandp203.dfs.core.windows.net/files/sales/csv/**',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW=TRUE\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": -1
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query delta table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://romandp203.dfs.core.windows.net/files/delta/products-delta/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n\n\nUSE AdventureWorks;\n\nSELECT * FROM Products;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "adventureworks",
						"poolName": "Built-in"
					},
					"resultLimit": -1
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark Transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "romandp203spark",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "romandp203",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "509c3d31-20e3-4332-ace4-cafbf7f296cf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e7fb6279-2ac9-42d6-9620-f4f2a25c1a12/resourceGroups/BD-SparkSandbox/providers/Microsoft.Synapse/workspaces/training-bd-cert/bigDataPools/romandp203spark",
						"name": "romandp203spark",
						"type": "Spark",
						"endpoint": "https://training-bd-cert.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/romandp203spark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "romandp203"
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Transform data by using Spark\r\n",
							"\r\n",
							"Apache Spark provides a distributed data processing platform that you can use to perform complex data transformations at scale."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service = \"abfss://files@romandp203.dfs.core.windows.net\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## To fix SQL execution on a non-default linked service\r\n",
							"\r\n",
							"```python\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Initialize Spark session\r\n",
							"spark = SparkSession.builder.getOrCreate()\r\n",
							"\r\n",
							"# Set the linked service name\r\n",
							"linked_service_name = 'files@romandp203'\r\n",
							"\r\n",
							"# Configure the Spark session to use the linked service\r\n",
							"spark.conf.set(\"spark.storage.synapse.linkedServiceName\", linked_service_name)\r\n",
							"spark.conf.set(\"spark.sql.warehouse.dir\", f\"abfss://{linked_service_name}.dfs.core.windows.net/synapse/workspaces/training-bd-cert/warehouse\")\r\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
							"```"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Load source data\n",
							"\n",
							"Let's start by loading some historical sales order data into a dataframe.\n",
							"\n",
							"Review the code in the cell below, which loads the sales order from all of the csv files within the **data** directory. Then click the **&#9655;** button to the left of the cell to run it.\n",
							"\n",
							"> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Print the Spark session configuration\r\n",
							"for item in spark.sparkContext.getConf().getAll():\r\n",
							"    if \"bdaadls\" in item[1]:\r\n",
							"        print(item)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"order_details = spark.read.csv(f'{linked_service}/data/*.csv', header=True, inferSchema=True)\n",
							"display(order_details.limit(5))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Transform the data structure\r\n",
							"\r\n",
							"The source data includes a **CustomerName** field, that contains the customer's first and last name. Let's modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split, col\r\n",
							"\r\n",
							"# Create the new FirstName and LastName fields\r\n",
							"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
							"\r\n",
							"# Remove the CustomerName field\r\n",
							"transformed_df = transformed_df.drop(\"CustomerName\")\r\n",
							"\r\n",
							"display(transformed_df.limit(5))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The code above creates a new dataframe with the **CustomerName** field removed and two new **FirstName** and **LastName** fields.\r\n",
							"\r\n",
							"You can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\r\n",
							"\r\n",
							"## Save the transformed data\r\n",
							"\r\n",
							"After making the required changes to the data, you can save the results in a supported file format.\r\n",
							"\r\n",
							"> **Note**: Commonly, *Parquet* format is preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\r\n",
							"\r\n",
							"Use the following code to save the transformed dataframe in Parquet format (Overwriting the data if it already exists)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transformed_df.write.parquet(f'{linked_service}/transformed_data/orders.parquet', mode=\"overwrite\")\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_data** has been created, containing a file named **orders.parquet**. Then return to this notebook."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Partition data\n",
							"\n",
							"A common way to optimize performance when dealing with large volumes of data is to partition the data files based on one or more field values. This can significant improve performance and make it easier to filter data.\n",
							"\n",
							"Use the following cell to derive new **Year** and **Month** fields and then save the resulting data in Parquet format, partitioned by year and month."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"dated_df = transformed_df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\r\n",
							"display(dated_df.limit(5))\r\n",
							"dated_df.write.parquet(f\"{linked_service}/partitioned_data\", mode=\"overwrite\", partitionBy=(\"Year\",\"Month\"))\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **partitioned_data** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\r\n",
							"\r\n",
							"You can read this data into a dataframe from any folder in the hierarchy, using explicit values or wildcards for partitioning fields. For example, use the following code to get the sales orders placed in 2020 for all months."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"orders_2020 = spark.read.parquet(f'{linked_service}/partitioned_data/Year=2020/Month=*')\r\n",
							"display(orders_2020.limit(5))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Note that the partitioning columns specified in the file path are omitted in the resulting dataframe.\r\n",
							"\r\n",
							"## Use SQL to transform data\r\n",
							"\r\n",
							"Spark is a very flexible platform, and the **SQL** library that provides the dataframe also enables you to work with data using SQL semantics. You can query and transform data in dataframes by using SQL queries, and persist the results as tables - which are metadata abstractions over files.\r\n",
							"\r\n",
							"First, use the following code to save the original sales orders data (loaded from CSV files) as a table. Technically, this is an *external* table because the **path** parameter is used to specify where the data files for the table are stored (an *internal* table is stored in the system storage for the Spark metastore and managed automatically)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path=f'{linked_service}/sales_orders_table')"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **sales_orders_table** has been created, containing parquet files for the table data. Then return to this notebook.\r\n",
							"\r\n",
							"Now that the table has been created, you can use SQL to transform it. For example, the following code derives new Year and Month columns and then saves the results as a partitioned external table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\r\n",
							"display(sql_transform.limit(5))\r\n",
							"sql_transform.write.saveAsTable('transformed_orders', format='parquet', mode='overwrite', partitionBy=(\"Year\",\"Month\"), path=f'{linked_service}/transformed_orders_table')"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_orders_table** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\n",
							"\n",
							"Essentially you've performed the same data transformation into partitioned parquet files as s before, but by using SQL instead of native dataframe methods.\n",
							"\n",
							"You can read this data into a dataframe from any folder in the hierarchy as before, but because the data files are also abstracted by a table in the metastore, you can query the data directly using SQL."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM transformed_orders\r\n",
							"WHERE Year = 2021\r\n",
							"    AND Month = 1"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Because these are *external* tables, you can drop the tables from the metastore without deleting the files - so the transfomed data remains available for other downstream data analytics or ingestion processes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DROP TABLE transformed_orders;\r\n",
							"DROP TABLE sales_orders;"
						],
						"outputs": [],
						"execution_count": 11
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/romandp203')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark delta lake exploration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "romandp203spark",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "romandp203",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "90b47254-3780-4e51-8da5-6b0194074ec2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e7fb6279-2ac9-42d6-9620-f4f2a25c1a12/resourceGroups/BD-SparkSandbox/providers/Microsoft.Synapse/workspaces/training-bd-cert/bigDataPools/romandp203spark",
						"name": "romandp203spark",
						"type": "Spark",
						"endpoint": "https://training-bd-cert.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/romandp203spark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "romandp203"
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Use Delta Lake with Spark in Azure Synapse Analytics"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"linked_service = \"abfss://files@romandp203.dfs.core.windows.net\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create delta tables"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load(f'{linked_service}/products/products.csv', format='csv', header=True)\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"delta_table_path = f\"{linked_service}/delta/products-delta\"\r\n",
							"df.write.format(\"delta\").save(delta_table_path, mode=\"overwrite\")"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"# Create a deltaTable object\r\n",
							"deltaTable = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							"# Update the table (reduce price of product 771 by 10%)\r\n",
							"deltaTable.update(\r\n",
							"    condition = \"ProductID == 771\",\r\n",
							"    set = { \"ListPrice\": \"ListPrice * 0.9\" })\r\n",
							"\r\n",
							"# View the updated data as a dataframe\r\n",
							"display(deltaTable.toDF().limit(10))"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"new_df = spark.read.load(delta_table_path, format=\"delta\")\r\n",
							"display(new_df.limit(10))"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"new_df = spark.read.load(delta_table_path, format=\"delta\", versionAsOf=0)\r\n",
							"display(new_df.limit(10))"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"deltaTable.history(10).show(20, False, True)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Create catalog tables\r\n",
							"\r\n",
							"### Create an external table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"CREATE DATABASE IF NOT EXISTS AdventureWorks\")\r\n",
							"spark.sql(f\"CREATE TABLE IF NOT EXISTS AdventureWorks.ProductsExternal USING DELTA LOCATION '{delta_table_path}'\")\r\n",
							"spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsExternal\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"USE AdventureWorks;\r\n",
							"SELECT * FROM ProductsExternal;"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Create a managed table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.write.format(\"delta\").saveAsTable(\"AdventureWorks.ProductsManaged\")\r\n",
							"spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsManaged\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"USE AdventureWorks;\r\n",
							"SELECT * FROM ProductsManaged;"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"USE AdventureWorks;\r\n",
							"SHOW TABLES;\r\n",
							"DROP TABLE IF EXISTS ProductsExternal;\r\n",
							"DROP TABLE IF EXISTS ProductsManaged;"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Create a table using SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"USE AdventureWorks;\r\n",
							"CREATE TABLE IF NOT EXISTS Products\r\n",
							"USING DELTA\r\n",
							"LOCATION '/delta/products-delta';"
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"USE AdventureWorks;\r\n",
							"SELECT * FROM Products;"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Use delta tables for streaming data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"# Create a folder\r\n",
							"inputPath = f'{linked_service}/streaming/'\r\n",
							"mssparkutils.fs.mkdirs(inputPath)\r\n",
							"\r\n",
							"# Create a stream that reads data from the folder, using a JSON schema\r\n",
							"jsonSchema = StructType([\r\n",
							"    StructField(\"device\", StringType(), False),\r\n",
							"    StructField(\"status\", StringType(), False)\r\n",
							"])\r\n",
							"iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\r\n",
							"\r\n",
							"# Write some event data to the folder\r\n",
							"device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
							"mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\r\n",
							"print(\"Source stream created...\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write the stream to a delta table\r\n",
							"delta_stream_table_path = f'{linked_service}/delta/iotdevicedata'\r\n",
							"checkpointpath = f'{linked_service}/delta/checkpoint'\r\n",
							"deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\r\n",
							"print(\"Streaming to delta sink...\")"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Read the data in delta format into a dataframe\r\n",
							"df = spark.read.format(\"delta\").load(delta_stream_table_path)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# create a catalog table based on the streaming sink\r\n",
							"spark.sql(f\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{delta_stream_table_path}'\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add more data to the source stream\r\n",
							"more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
							"\r\n",
							"mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"deltastream.stop()"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/romandp203')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark exploration')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "romandp203spark",
					"type": "BigDataPoolReference"
				},
				"targetSparkConfiguration": {
					"referenceName": "romandp203",
					"type": "SparkConfigurationReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4a6932ba-141b-4968-ad6e-9744c71e0d3e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e7fb6279-2ac9-42d6-9620-f4f2a25c1a12/resourceGroups/BD-SparkSandbox/providers/Microsoft.Synapse/workspaces/training-bd-cert/bigDataPools/romandp203spark",
						"name": "romandp203spark",
						"type": "Spark",
						"endpoint": "https://training-bd-cert.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/romandp203spark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30,
					"targetSparkConfiguration": "romandp203"
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"orderSchema = StructType([\r\n",
							"    StructField(\"SalesOrderNumber\", StringType()),\r\n",
							"    StructField(\"SalesOrderLineNumber\", IntegerType()),\r\n",
							"    StructField(\"OrderDate\", DateType()),\r\n",
							"    StructField(\"CustomerName\", StringType()),\r\n",
							"    StructField(\"Email\", StringType()),\r\n",
							"    StructField(\"Item\", StringType()),\r\n",
							"    StructField(\"Quantity\", IntegerType()),\r\n",
							"    StructField(\"UnitPrice\", FloatType()),\r\n",
							"    StructField(\"Tax\", FloatType())\r\n",
							"    ])\r\n",
							"\r\n",
							"df = spark.read.load(\r\n",
							"    'abfss://files@romandp203.dfs.core.windows.net/sales/orders/*.csv',\r\n",
							"    format='csv',\r\n",
							"    schema=orderSchema\r\n",
							")\r\n",
							"display(df.limit(100))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df.toPandas())"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"customers = df['CustomerName', 'Email']\r\n",
							"print(customers.count())\r\n",
							"print(customers.distinct().count())\r\n",
							"display(customers.distinct())"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"customers = df.select(\"CustomerName\", \"Email\").where(df['Item']=='Road-250 Red, 52')\r\n",
							"print(customers.count())\r\n",
							"print(customers.distinct().count())\r\n",
							"display(customers.distinct())"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"productSales = df.select(\"Item\", \"Quantity\").groupBy(\"Item\").sum()\r\n",
							"display(productSales)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"yearlySales = df.select(year(\"OrderDate\").alias(\"Year\")).groupBy(\"Year\").count().orderBy(\"Year\")\r\n",
							"display(yearlySales)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df.createOrReplaceTempView(\"salesorders\")\r\n",
							"spark_df = spark.sql(\"SELECT * FROM salesorders\")\r\n",
							"display(spark_df)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT YEAR(OrderDate) AS OrderYear,\r\n",
							"       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\r\n",
							"FROM salesorders\r\n",
							"GROUP BY YEAR(OrderDate)\r\n",
							"ORDER BY OrderYear;"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT * FROM salesorders"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\r\n",
							"                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\r\n",
							"            FROM salesorders \\\r\n",
							"            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\r\n",
							"            ORDER BY OrderYear\"\r\n",
							"df_spark = spark.sql(sqlQuery)\r\n",
							"df_spark.show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from matplotlib import pyplot as plt\r\n",
							"\r\n",
							"df_sales = df_spark.toPandas()\r\n",
							"\r\n",
							"fig, ax = plt.subplots(1, 2, figsize = (10,4))\r\n",
							"\r\n",
							"ax[0].bar(df_sales[\"OrderYear\"], df_sales[\"GrossRevenue\"], color=\"orange\")\r\n",
							"ax[0].set_title('Revenue by Year')\r\n",
							"\r\n",
							"yearly_counts = df_sales['OrderYear'].value_counts()\r\n",
							"ax[1].pie(yearly_counts)\r\n",
							"ax[1].set_title('Orders per Year')\r\n",
							"ax[1].legend(yearly_counts.keys().tolist())\r\n",
							"\r\n",
							"fig.suptitle('Sales Data')\r\n",
							"\r\n",
							"ax[0].set_title(\"Revenue by year\")\r\n",
							"ax[0].set_xlabel(\"Year\")\r\n",
							"ax[0].set_ylabel(\"Revenue\")"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import seaborn as sns\r\n",
							"\r\n",
							"sns.set_theme(style=\"whitegrid\")\r\n",
							"ax = sns.lineplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkConfigurations/romandp203')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/romandp203')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "Used to work on a warehouse linked to the non-default linked service\n",
				"configs": {
					"spark.storage.synapse.linkedServiceName": "romandp203",
					"spark.sql.warehouse.dir": "abfss://files@romandp203.dfs.core.windows.net/synapse/workspaces/training-bd-cert/warehouse",
					"fs.azure.account.oauth.provider.type": "com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider"
				},
				"created": "2024-09-16T13:23:59.7780000+02:00",
				"createdBy": "rrieunier@adflux.net",
				"annotations": [
					""
				],
				"configMergeRule": {
					"artifact.currentOperation.spark.storage.synapse.linkedServiceName": "replace",
					"artifact.currentOperation.spark.sql.warehouse.dir": "replace",
					"artifact.currentOperation.fs.azure.account.oauth.provider.type": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RetailDB')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "RetailDB",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true,
								"DerivedModelDBInfo": "{\"ModelDirectives\":{\"BaseModel\":{\"Name\":\"Retail\",\"Version\":\"1.3.0\"}}}"
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://files@romandp203.dfs.core.windows.net/RetailDB",
								"Properties": {
									"FormatType": "csv",
									"LinkedServiceName": "romandp203"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "Customer",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "CustomerId",
										"Description": "Unique customer ID",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "FirstName",
										"Description": "Customer first name",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "LastName",
										"Description": "Customer last name",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "EmailAddress",
										"Description": "Customer email",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Phone",
										"Description": "Customer phone",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/Customer",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"escape": "\\",
										"quote": "\"",
										"FormatTypeSetToDatabaseDefault": true,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/Customer",
									"Properties": {
										"LinkedServiceName": "romandp203",
										"LocationSetToDatabaseDefault": true
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "CustomerId",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "Product",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\n\nThere are two basic types of products:\n\n- Tangible (physical)\n- Intangible (non-physical) such as services\n\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\n\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\n\nA product typically goes through five stages of development:\n\n(1) Idea Stage - involving a thorough evaluation of the potential product\n\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\n\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\n\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\n\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\n\nProducts tend to be categorized as either:  Industrial goods and consumer goods\n\nIndustrial goods are used to produce other products .\n\nIndustrial goods may be further divided into:\n\n- Raw materials\n- Equipment\n- Pre-built materials \n- Supplies.\n\nConsumer goods are intended for consumption by the general public.\n\nConsumer goods may be further divided into:\n\n- Durable goods\n- Nondurable goods\n- Packaged goods\n\nA product may be a member of a product family or product line.\n\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\n\nMembers of a product family frequently have many common parts and assemblies.\n\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\n\nEx:\nThe Apple Macintosh family of products consists of the product lines:\n- Mac mini\n- MacBook Pro\n- Mac Pro\n\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\n\nA Product Family typically is created to address one or five functions:\n\n1. To increase profits and not erode the sales of existing products\n\n2. To attract additional Markets or Market Segments\n\n3. To counter competitor's products\n\n4. To fill a gap in an existing Product Family.\n\n5. To promote sales of other products in the family line\n\nLine Depth refers to the number of products in the product line.\n\nLine consistency refers to how closely related the products are that make up the product line.\n\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\n\nProduct width refers to the number of different product lines sold by a company.\n\nProduct mix refers to the total number of products sold in all product lines.\n\nLine extension refers to the adding of a new product to a line.\n\n\"Trading up or brand leveraging\" refers to adding a product of better quality to a product line than the other products in that line.\n\n\"Trading down\" refers to adding a product of lesser quality to a product line than the other products in that line.\n\nIf a line of products is sold with the same brand name, this is referred to as family branding.\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\nProduct-line managers typically have the following responsibilities:\n- Expansion and composition of a product line\n- Evaluate the effects of product mixes on the profitability of other items in the line\n- Planning and allocation of resources to individual products in the line\nA product is normally associated with a brand strategy - manufacturer, private or generic.\n\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\n\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\n\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\n\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\n\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\n\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\n\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\n\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\n\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\n\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\n\nBy contrast, family branding has several advantages.\n\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\n\nReduced or eliminated time for name identification and advertising for name recognition purposes.",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "ProductId",
										"Description": "The unique identifier of a Product.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductId"
										}
									},
									{
										"Name": "ProductName",
										"Description": "The name of the Product, which normally corresponds to the 'marketing name' of the Product.",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductName"
										}
									},
									{
										"Name": "IntroductionDate",
										"Description": "The date that the Product was introduced for sale.",
										"OriginDataTypeName": {
											"TypeName": "date",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"DateFormat": "YYYY-MM-DD",
												"HIVE_TYPE_STRING": "date"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "IntroductionDate"
										}
									},
									{
										"Name": "ActualAbandonmentDate",
										"Description": "The actual date that the marketing of the product was discontinued. \n\nAbandonment is a component in the decline stage of the product's life cycle characterized by a reduced market demand for the product and an increased number of competing products with similar characteristics.\n\nThere are three (3) strategies for abandoning a product:\n\n(1)  Reduced marketing and expenditures to maintain profits.\n\n(2)  Concentrating on the strongest market segments and eliminating the weaker market segments\n\n(3)  Maintain the marketing level until the product is discontinued.",
										"OriginDataTypeName": {
											"TypeName": "date",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"DateFormat": "YYYY-MM-DD",
												"HIVE_TYPE_STRING": "date"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ActualAbandonmentDate"
										}
									},
									{
										"Name": "ProductGrossWeight",
										"Description": "The gross product weight.",
										"OriginDataTypeName": {
											"TypeName": "decimal",
											"IsComplexType": false,
											"IsNullable": true,
											"Precision": 18,
											"Scale": 8,
											"Properties": {
												"HIVE_TYPE_STRING": "decimal"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductGrossWeight"
										}
									},
									{
										"Name": "ItemSku",
										"Description": "The Stock Keeping Unit identifier, which is typically used for inventory-related activities.",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 40,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ItemSku"
										}
									},
									{
										"Name": "ListPrice",
										"Description": "The product price.",
										"OriginDataTypeName": {
											"TypeName": "decimal",
											"IsComplexType": false,
											"IsNullable": false,
											"Precision": 18,
											"Scale": 2,
											"Properties": {
												"HIVE_TYPE_STRING": "decimal"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/Product",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"escape": "\\",
										"quote": "\"",
										"FormatTypeSetToDatabaseDefault": true,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/Product",
									"Properties": {
										"LinkedServiceName": "romandp203",
										"LocationSetToDatabaseDefault": true
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{\"ProductId\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductId\"},\"ProductName\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductName\"},\"IntroductionDate\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"IntroductionDate\"},\"ActualAbandonmentDate\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ActualAbandonmentDate\"},\"ProductGrossWeight\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductGrossWeight\"},\"ItemSku\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ItemSku\"}}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"DerivedModelEntityInfo": "{\"entityDirectives\":{\"name\":\"Product\",\"description\":\"A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\\n\\nThere are two basic types of products:\\n\\n- Tangible (physical)\\n- Intangible (non-physical) such as services\\n\\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\\n\\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\\n\\nA product typically goes through five stages of development:\\n\\n(1) Idea Stage - involving a thorough evaluation of the potential product\\n\\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\\n\\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\\n\\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\\n\\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\\n\\nProducts tend to be categorized as either:  Industrial goods and consumer goods\\n\\nIndustrial goods are used to produce other products .\\n\\nIndustrial goods may be further divided into:\\n\\n- Raw materials\\n- Equipment\\n- Pre-built materials \\n- Supplies.\\n\\nConsumer goods are intended for consumption by the general public.\\n\\nConsumer goods may be further divided into:\\n\\n- Durable goods\\n- Nondurable goods\\n- Packaged goods\\n\\nA product may be a member of a product family or product line.\\n\\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\\n\\nMembers of a product family frequently have many common parts and assemblies.\\n\\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\\n\\nEx:\\nThe Apple Macintosh family of products consists of the product lines:\\n- Mac mini\\n- MacBook Pro\\n- Mac Pro\\n\\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\\n\\nA Product Family typically is created to address one or five functions:\\n\\n1. To increase profits and not erode the sales of existing products\\n\\n2. To attract additional Markets or Market Segments\\n\\n3. To counter competitor's products\\n\\n4. To fill a gap in an existing Product Family.\\n\\n5. To promote sales of other products in the family line\\n\\nLine Depth refers to the number of products in the product line.\\n\\nLine consistency refers to how closely related the products are that make up the product line.\\n\\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\\n\\nProduct width refers to the number of different product lines sold by a company.\\n\\nProduct mix refers to the total number of products sold in all product lines.\\n\\nLine extension refers to the adding of a new product to a line.\\n\\n\\\"Trading up or brand leveraging\\\" refers to adding a product of better quality to a product line than the other products in that line.\\n\\n\\\"Trading down\\\" refers to adding a product of lesser quality to a product line than the other products in that line.\\n\\nIf a line of products is sold with the same brand name, this is referred to as family branding.\\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\\nProduct-line managers typically have the following responsibilities:\\n- Expansion and composition of a product line\\n- Evaluate the effects of product mixes on the profitability of other items in the line\\n- Planning and allocation of resources to individual products in the line\\nA product is normally associated with a brand strategy - manufacturer, private or generic.\\n\\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\\n\\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\\n\\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\\n\\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\\n\\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\\n\\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\\n\\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\\n\\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\\n\\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\\n\\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\\n\\nBy contrast, family branding has several advantages.\\n\\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\\n\\nReduced or eliminated time for name identification and advertising for name recognition purposes.\",\"baseEntityReference\":{\"name\":\"RetailProduct\",\"path\":\"RetailProduct.cdm.json/RetailProduct\"},\"primaryKey\":[\"ProductId\"],\"projectionInfo\":{\"attributes\":[{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductId\"},\"dataType\":\"long\",\"description\":\"The unique identifier of a Product.\",\"isNullable\":false,\"name\":\"ProductId\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductName\"},\"dataType\":\"string\",\"dataTypeLength\":256,\"description\":\"The name of the Product, which normally corresponds to the 'marketing name' of the Product.\",\"isNullable\":true,\"name\":\"ProductName\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"IntroductionDate\"},\"dataType\":\"date\",\"dateFormat\":\"YYYY-MM-DD\",\"description\":\"The date that the Product was introduced for sale.\",\"isNullable\":true,\"name\":\"IntroductionDate\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ActualAbandonmentDate\"},\"dataType\":\"date\",\"dateFormat\":\"YYYY-MM-DD\",\"description\":\"The actual date that the marketing of the product was discontinued. \\n\\nAbandonment is a component in the decline stage of the product's life cycle characterized by a reduced market demand for the product and an increased number of competing products with similar characteristics.\\n\\nThere are three (3) strategies for abandoning a product:\\n\\n(1)  Reduced marketing and expenditures to maintain profits.\\n\\n(2)  Concentrating on the strongest market segments and eliminating the weaker market segments\\n\\n(3)  Maintain the marketing level until the product is discontinued.\",\"isNullable\":true,\"name\":\"ActualAbandonmentDate\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductGrossWeight\"},\"dataType\":\"decimal\",\"dataTypeLength\":18,\"description\":\"The gross product weight.\",\"isNullable\":true,\"scale\":8,\"name\":\"ProductGrossWeight\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ItemSku\"},\"dataType\":\"string\",\"dataTypeLength\":40,\"description\":\"The Stock Keeping Unit identifier, which is typically used for inventory-related activities.\",\"isNullable\":true,\"name\":\"ItemSku\"},{\"type\":\"New\",\"dataType\":\"decimal\",\"dataTypeLength\":18,\"description\":\"The product price.\",\"isNullable\":false,\"scale\":2,\"name\":\"ListPrice\"}]}}}",
								"Description": "A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\n\nThere are two basic types of products:\n\n- Tangible (physical)\n- Intangible (non-physical) such as services\n\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\n\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\n\nA product typically goes through five stages of development:\n\n(1) Idea Stage - involving a thorough evaluation of the potential product\n\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\n\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\n\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\n\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\n\nProducts tend to be categorized as either:  Industrial goods and consumer goods\n\nIndustrial goods are used to produce other products .\n\nIndustrial goods may be further divided into:\n\n- Raw materials\n- Equipment\n- Pre-built materials \n- Supplies.\n\nConsumer goods are intended for consumption by the general public.\n\nConsumer goods may be further divided into:\n\n- Durable goods\n- Nondurable goods\n- Packaged goods\n\nA product may be a member of a product family or product line.\n\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\n\nMembers of a product family frequently have many common parts and assemblies.\n\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\n\nEx:\nThe Apple Macintosh family of products consists of the product lines:\n- Mac mini\n- MacBook Pro\n- Mac Pro\n\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\n\nA Product Family typically is created to address one or five functions:\n\n1. To increase profits and not erode the sales of existing products\n\n2. To attract additional Markets or Market Segments\n\n3. To counter competitor's products\n\n4. To fill a gap in an existing Product Family.\n\n5. To promote sales of other products in the family line\n\nLine Depth refers to the number of products in the product line.\n\nLine consistency refers to how closely related the products are that make up the product line.\n\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\n\nProduct width refers to the number of different product lines sold by a company.\n\nProduct mix refers to the total number of products sold in all product lines.\n\nLine extension refers to the adding of a new product to a line.\n\n\"Trading up or brand leveraging\" refers to adding a product of better quality to a product line than the other products in that line.\n\n\"Trading down\" refers to adding a product of lesser quality to a product line than the other products in that line.\n\nIf a line of products is sold with the same brand name, this is referred to as family branding.\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\nProduct-line managers typically have the following responsibilities:\n- Expansion and composition of a product line\n- Evaluate the effects of product mixes on the profitability of other items in the line\n- Planning and allocation of resources to individual products in the line\nA product is normally associated with a brand strategy - manufacturer, private or generic.\n\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\n\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\n\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\n\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\n\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\n\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\n\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\n\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\n\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\n\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\n\nBy contrast, family branding has several advantages.\n\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\n\nReduced or eliminated time for name identification and advertising for name recognition purposes.",
								"DisplayFolderInfo": "{\"name\":\"Product\",\"colorCode\":\"#BD4B37\"}",
								"PrimaryKeys": "ProductId",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "SalesOrder",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "SalesOrderId",
										"Description": "The unique identifier of an order",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "OrderDate",
										"Description": "The date of the order.",
										"OriginDataTypeName": {
											"TypeName": "timestamp",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"TimestampFormat": "YYYY-MM-DD HH:MM:SS.fffffffff",
												"HIVE_TYPE_STRING": "timestamp"
											}
										}
									},
									{
										"Name": "LineItemId",
										"Description": "The ID of an individual line item.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "CustomerId",
										"Description": "The customer.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "ProductId",
										"Description": "The product.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "Quantity",
										"Description": "The order quantity.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/SalesOrder",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@romandp203.dfs.core.windows.net/RetailDB/SalesOrder",
									"Properties": {
										"LinkedServiceName": "romandp203",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "SalesOrderId,LineItemId",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "relationship-bckvmsbztt",
							"EntityType": "RELATIONSHIP",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Origin": {
								"Type": "SPARK"
							},
							"FromTableName": "Customer",
							"ToTableName": "SalesOrder",
							"ColumnRelationshipInformations": [
								{
									"FromColumnName": "CustomerId",
									"ToColumnName": "CustomerId"
								}
							],
							"RelationshipType": 0,
							"Properties": {}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "relationship-oboxsgnmsn",
							"EntityType": "RELATIONSHIP",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Origin": {
								"Type": "SPARK"
							},
							"FromTableName": "Product",
							"ToTableName": "SalesOrder",
							"ColumnRelationshipInformations": [
								{
									"FromColumnName": "ProductId",
									"ToColumnName": "ProductId"
								}
							],
							"RelationshipType": 0,
							"Properties": {}
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/romandp203spark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/romandp203')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "westeurope"
		}
	]
}